{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73c21bb-77e6-4562-849d-4769acdb9fdf",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff62f95-eb71-4d54-a81f-a72d3608fb42",
   "metadata": {},
   "source": [
    "\n",
    "$Y_i = \\beta x_i + \\alpha + \\epsilon$\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f37a3-bc4e-41e1-8437-b22eff14f618",
   "metadata": {},
   "source": [
    "Where $y_i$ is the number of minutes user $i$ spends on the site daily, $x_i$ is the number of friends user $i$ has, and $\\epsilon$ is a (hopefully small) error term representing the fact that there are other factors not accounted for this by this simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52ef6df-63f4-4a0d-a126-68f488aafc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alpha: float, beta: float, x_i: float) -> float:\n",
    "    return beta * x_i + alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ff409c-1235-4778-bb55-0c2c269d017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha: float, beta: float, x_i: float, y_i: float) -> float:\n",
    "    \"\"\"\n",
    "    The error from predicting beta * x_i + alpha\n",
    "    when the actual value is y_i\n",
    "    \"\"\"\n",
    "    return predict(alpha, beta, x_i) - y_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e55ed1-8cd7-4de6-94c1-243ddd5b41c2",
   "metadata": {},
   "source": [
    "add up the **squared** errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f132c4bf-f92e-4087-8b6c-f78d5262e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb6a4b3-b4a2-4f8c-8ec3-78719cc56711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_sqerrors(alpha: float, beta: float, x: Vector, y: Vector) -> float:\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2\n",
    "               for x_i, y_i in zip(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5dbb0-5874-496e-8466-c7cc4082ee9c",
   "metadata": {},
   "source": [
    "The **least squares** solution is to choose the alpha and beta that make sum_of_sqerrors as small as possible.\n",
    "Using calculus (or tedious algebra), the error-minimizing alpha and beta are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09235f9c-2a86-4d55-a291-d1898d5e699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from scratch.linear_algebra import Vector\n",
    "from scratch.statistics import correlation, standard_deviation, mean\n",
    "\n",
    "def least_squares_fit(x: Vector, y: Vector) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Given two vectors x and y,\n",
    "    find the least-squares values of alpha and beta\n",
    "    \"\"\"\n",
    "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62952d5-c2ee-4129-b6fa-aefa069255b9",
   "metadata": {},
   "source": [
    "The choice of alpha simply says that when we see the average value of the independent variable x, we predict the average value of the dependent variable y.\n",
    "The choice of beta means that when the input value increases by standard_deviation(x), the prediction then increases by\n",
    "\n",
    "correlation(x, y) * standard_deviation(y). \n",
    "\n",
    "In the case where x and y are perfectly correlated, a one-standard-deviation increase in x results in a one-standard-deviation-of-y increase in the prediction. When they’re perfectly anticorrelated, the increase in x results in a decrease in the prediction. \n",
    "\n",
    "When correlation is 0, beta is 0, which means that changes in x dont affect the prediction at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc07a6e-0725-4da4-936f-dba3014ba3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(-100, 110, 10)]\n",
    "y = [3 * i - 5 for i in x]\n",
    "\n",
    "# Should find that y = 3x - 5\n",
    "assert least_squares_fit(x, y) == (-5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48e50c0b-0f07-4229-b03f-ee734bf8869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.statistics import num_friends_good, daily_minutes_good\n",
    "\n",
    "alpha, beta = least_squares_fit(num_friends_good, daily_minutes_good)\n",
    "assert 22.9 < alpha < 23.0\n",
    "assert 0.9 < beta < 0.905"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b546f-4163-4c56-9ca8-c8e3564fd272",
   "metadata": {},
   "source": [
    "A common measure is the **coefficient of determination** (or **R-squared**), which measures the graction of the total variation in the dependent variable that is captured by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67513dbc-a152-471f-8c56-b0e196b4453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.statistics import de_mean\n",
    "\n",
    "def total_sum_of_squares(y: Vector) -> float:\n",
    "    \"\"\"the total squared variation of y_i's from their mean\"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))\n",
    "\n",
    "def r_squared(alpha: float, beta: float, x: Vector, y: Vector) -> float:\n",
    "    \"\"\"\n",
    "    the fraction of variation in y captured by the model, which equals\n",
    "    1 - the fraction of variation in y not captured by the model\n",
    "    \"\"\"\n",
    "    return 1.0 - (sum_of_sqerrors(alpha, beta, x, y) /\n",
    "                  total_sum_of_squares(y))\n",
    "\n",
    "rsq = r_squared(alpha, beta, num_friends_good, daily_minutes_good)\n",
    "assert 0.328 < rsq < 0.330"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0182a16-b245-481b-93f4-34bef7f58c11",
   "metadata": {},
   "source": [
    "## Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0de9102-8ff0-4952-b3d0-b6aa958a017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 13196.619: 100%|██████████| 10000/10000 [00:56<00:00, 177.32it/s]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    import random\n",
    "    import tqdm\n",
    "    from scratch.gradient_descent import gradient_step\n",
    "    \n",
    "    num_epochs = 10000\n",
    "    random.seed(0)\n",
    "    \n",
    "    guess = [random.random(), random.random()]  # choose random value to start\n",
    "    \n",
    "    learning_rate = 0.00001\n",
    "    \n",
    "    with tqdm.trange(num_epochs) as t:\n",
    "        for _ in t:\n",
    "            alpha, beta = guess\n",
    "    \n",
    "            # Partial derivative of loss with respect to alpha\n",
    "            grad_a = sum(2 * error(alpha, beta, x_i, y_i)\n",
    "                         for x_i, y_i in zip(num_friends_good,\n",
    "                                             daily_minutes_good))\n",
    "    \n",
    "            # Partial derivative of loss with respect to beta\n",
    "            grad_b = sum(2 * error(alpha, beta, x_i, y_i) * x_i\n",
    "                         for x_i, y_i in zip(num_friends_good,\n",
    "                                             daily_minutes_good))\n",
    "    \n",
    "            # Compute loss to stick in the tqdm description\n",
    "            loss = sum_of_sqerrors(alpha, beta,\n",
    "                                   num_friends_good, daily_minutes_good)\n",
    "            t.set_description(f\"loss: {loss:.3f}\")\n",
    "    \n",
    "            # Finally, update the guess\n",
    "            guess = gradient_step(guess, [grad_a, grad_b], -learning_rate)\n",
    "    \n",
    "    # We should get pretty much the same results:\n",
    "    alpha, beta = guess\n",
    "    assert 22.9 < alpha < 23.0\n",
    "    assert 0.9 < beta < 0.905\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3e153-6a0d-40ce-944e-231c9869e50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
